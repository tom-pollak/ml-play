# %%
# Chapter 1 Interprebiltiy
## Transformer from Scratch
from __future__ import annotations
import os

os.environ["ACCELERATE_DISABLE_RICH"] = "1"
import sys
import einops
from dataclasses import dataclass
from transformer_lens import HookedTransformer
from transformer_lens.utils import gelu_new, tokenize_and_concatenate
import torch as t
from torch import Tensor
import torch.nn as nn
import numpy as np
import math
from tqdm.notebook import tqdm
from typing import Tuple, List, Optional, Dict, Callable
from jaxtyping import Float, Int
from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast
from collections import defaultdict
from rich.table import Table
from rich import print as rprint
import datasets
from torch.utils.data import DataLoader, default_collate
import wandb
from pathlib import Path
import webbrowser
import torch.nn.functional as F

# Make sure exercises are in the path
# chapter = r"chapter1_transformer_interp"
# exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
# section_dir = exercises_dir / "part1_transformer_from_scratch"
# if str(exercises_dir) not in sys.path:
#     sys.path.append(str(exercises_dir))
# import part1_transformer_from_scratch.solutions as solutions
# from plotly_utils import imshow

device = t.device(
    "cuda"
    if t.cuda.is_available()
    else "mps" if t.backends.mps.is_available() else "cpu"
)

MAIN = __name__ == "__main__"

reference_gpt2 = HookedTransformer.from_pretrained(
    "gpt2-small",
    fold_ln=False,
    center_unembed=False,
    center_writing_weights=False,
)

from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
inputs = tokenizer("Hello World", return_tensors="pt")
outputs = model(**inputs, output_hidden_states=True)

# %%


@t.inference_mode()
def generate(input, model, tokenizer, n, top_p=50, temp=0.0):
    t.manual_seed(1337)
    temp = max(temp, 1e-8)
    out = []

    tokens = tokenizer(
        input,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=model.cfg.n_ctx,
    )["input_ids"].to(device)

    for i in range(n):
        logits = model(tokens)[0, -1]
        vals, idxs = logits.topk(top_p)
        probs = F.softmax(vals / temp, dim=-1)
        next_token = idxs[t.multinomial(probs, 1)]
        tokens = t.cat([tokens, next_token[None]], dim=1)
        out.append(tokenizer.decode(tokens[0]))
    return out


for temp in t.arange(0, 2, 0.1):
    print(
        f"{temp.item():.1f}:",
        generate(
            "I am a large language model. My objective is:",
            reference_gpt2,
            reference_gpt2.tokenizer,
            n=25,
            top_p=50,
            temp=temp.item(),
        )[-1],
    )

# %%
tokens = reference_gpt2.to_tokens(
    [
        "I am a large language model. My objective is: ",
        "Hello, my name is",
        "What's up my dudes!",
    ]
).to(device)
logits, cache = reference_gpt2.run_with_cache(tokens)
# %%

print("ACTIVATIONS")
for name, activation in cache.items():
    if ".0." in name or "blocks" not in name:
        print(f"{name:30} {tuple(activation.shape)}")

print("\nPARAMETERS")
for name, param in reference_gpt2.named_parameters():
    if ".0." in name or "blocks" not in name:
        print(f"{name:18} {tuple(param.shape)}")

# %%

from dataclasses import dataclass


@dataclass
class Config:
    d_model: int = 768
    debug: bool = True
    layer_norm_eps: float = 1e-5
    d_vocab: int = 50257
    init_range: float = 0.02
    n_ctx: int = 1024
    d_head: int = 64  # (d_model // n_heads)
    d_mlp: int = 3072  # (d_model * 4)
    n_heads: int = 12
    n_layers: int = 12


def rand_float_test(cls, shape):
    cfg = Config(debug=True)
    layer = cls(cfg).to(device)
    random_input = t.randn(shape).to(device)
    print("Input shape:", random_input.shape)
    output = layer(random_input)
    if isinstance(output, tuple):
        output = output[0]
    print("Output shape:", output.shape, "\n")


def rand_int_test(cls, shape):
    cfg = Config(debug=True)
    layer = cls(cfg).to(device)
    random_input = t.randint(100, 1000, shape).to(device)
    print("Input shape:", random_input.shape)
    output = layer(random_input)
    if isinstance(output, tuple):
        output = output[0]
    print("Output shape:", output.shape, "\n")


def load_gpt2_test(cls, gpt2_layer, input):
    cfg = Config(debug=True)
    layer = cls(cfg).to(device)
    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)
    print("Input shape:", input.shape)
    output = layer(input)
    if isinstance(output, tuple):
        output = output[0]
    print("Output shape:", output.shape)
    try:
        reference_output = gpt2_layer(input)
    except:
        reference_output = gpt2_layer(input, input, input)
    print("Reference output shape:", reference_output.shape, "\n")
    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)
    print(f"{comparison.sum()/comparison.numel():.2%} of the values are correct\n")


# %%


class LayerNorm(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.w = nn.Parameter(t.ones(cfg.d_model))
        self.b = nn.Parameter(t.zeros(cfg.d_model))
        self.eps = cfg.layer_norm_eps

    def forward(
        self, residual: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        mean = residual.mean(dim=-1, keepdim=True)
        std = (residual.var(dim=-1, keepdim=True, correction=0) + self.eps).sqrt()
        residual = (residual - mean) / std
        residual = self.w * residual + self.b
        return residual


class Embed(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.W_E = nn.Parameter(t.randn(cfg.d_vocab, cfg.d_model) * cfg.init_range)

    def forward(
        self, tokens: Int[Tensor, "batch position"]
    ) -> Float[Tensor, "batch position d_model"]:
        return self.W_E[tokens]


class PosEmbed(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.W_pos = nn.Parameter(t.randn(cfg.n_ctx, cfg.d_model) * cfg.init_range)

    def forward(
        self, tokens: Int[Tensor, "batch position"]
    ) -> Float[Tensor, "batch position d_model"]:
        B, T = tokens.shape
        return self.W_pos[:T].repeat(B, 1, 1)  # explict, but can be broadcasted


class Attention(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        QKV_shape = (cfg.n_heads, cfg.d_model, cfg.d_head)
        self.d_head = cfg.d_head
        self.W_Q = nn.Parameter(t.randn(QKV_shape) * cfg.init_range)
        self.W_K = nn.Parameter(t.randn(QKV_shape) * cfg.init_range)
        self.W_V = nn.Parameter(t.randn(QKV_shape) * cfg.init_range)

        QKV_b_shape = (cfg.n_heads, cfg.d_head)
        self.b_Q = nn.Parameter(t.zeros(QKV_b_shape))
        self.b_K = nn.Parameter(t.zeros(QKV_b_shape))
        self.b_V = nn.Parameter(t.zeros(QKV_b_shape))

        O_shape = (cfg.n_heads, cfg.d_head, cfg.d_model)
        self.W_O = nn.Parameter(t.randn(O_shape) * cfg.init_range)
        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))

    def apply_causal_mask(
        self, attn_scores: Float[Tensor, "batch n_heads query_pos key_pos"]
    ) -> Float[Tensor, "batch n_heads query_pos key_pos"]:
        B, H, QT, KT = attn_scores.shape
        mask = t.triu(t.ones(QT, KT, device=device), diagonal=1).bool()
        attn_scores.masked_fill_(mask, float("-inf"))
        return attn_scores

    def forward(
        self, x: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        qkv_tfm = (
            "batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head"
        )
        q = einops.einsum(x, self.W_Q, qkv_tfm) + self.b_Q
        k = einops.einsum(x, self.W_K, qkv_tfm) + self.b_K
        v = einops.einsum(x, self.W_V, qkv_tfm) + self.b_V

        attn_tfm = "batch q_pos n_heads d_head, batch k_pos n_heads d_head -> batch n_heads q_pos k_pos"
        attn = einops.einsum(q, k, attn_tfm) / (self.d_head**0.5)
        attn_masked = self.apply_causal_mask(attn)
        attn_masked = attn_masked.softmax(dim=-1)

        z_tfm = "batch k_pos n_heads d_head, batch n_heads q_pos k_pos -> batch q_pos n_heads d_head"
        z = einops.einsum(v, attn_masked, z_tfm)

        up_proj_tfm = (
            "batch q_pos n_heads d_head, n_heads d_head d_model -> batch q_pos d_model"
        )
        up_proj = einops.einsum(z, self.W_O, up_proj_tfm) + self.b_O
        return up_proj


class MLP(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.W_in = nn.Parameter(t.randn(cfg.d_model, cfg.d_mlp) * cfg.init_range)
        self.W_out = nn.Parameter(t.randn(cfg.d_mlp, cfg.d_model) * cfg.init_range)
        self.b_in = nn.Parameter(t.zeros(cfg.d_mlp))
        self.b_out = nn.Parameter(t.zeros(cfg.d_model))

    def forward(
        self, norm_x: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        h = norm_x @ self.W_in + self.b_in  # batch posn d_mlp
        h = self.gelu(h)
        out = h @ self.W_out + self.b_out  # batch posn d_model
        return out

    @staticmethod
    def gelu(
        input: Float[Tensor, "batch pos d_mlp"]
    ) -> Float[Tensor, "batch pos d_mlp"]:
        "GeLU used by GPT2"
        return (
            0.5
            * input
            * (
                1.0
                + t.tanh(np.sqrt(2.0 / np.pi) * (input + 0.044715 * t.pow(input, 3.0)))
            )
        )


class TransformerBlock(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.ln1 = LayerNorm(cfg)
        self.attn = Attention(cfg)
        self.ln2 = LayerNorm(cfg)
        self.mlp = MLP(cfg)

    def forward(
        self, residual: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        x = residual
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x


class Unembed(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.W_U = nn.Parameter(t.randn(cfg.d_model, cfg.d_vocab) * cfg.init_range)
        self.b_U = nn.Parameter(t.zeros(cfg.d_vocab))  # requires_grad = False??

    def forward(
        self, residual: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_vocab"]:
        logits = residual @ self.W_U + self.b_U
        return logits


class DemoTransformer(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.embed = Embed(cfg)
        self.pos_embed = PosEmbed(cfg)
        self.blocks = nn.ModuleList(
            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]
        )
        self.ln_final = LayerNorm(cfg)
        self.unembed = Unembed(cfg)

    def forward(
        self, tokens: Int[Tensor, "batch position"]
    ) -> Float[Tensor, "batch position d_vocab"]:
        x = self.embed(tokens) + self.pos_embed(tokens)
        for block in self.blocks:
            x = block(x)
        x = self.ln_final(x)
        x = self.unembed(x)
        return x


demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)
demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)

demo_logits = demo_gpt2(tokens)
demo_logits.shape

# %%


def get_log_probs(
    logits: Float[Tensor, "batch posn d_vocab"], tokens: Int[Tensor, "batch posn"]
) -> Float[Tensor, "batch posn-1"]:
    log_probs = logits.log_softmax(dim=-1)
    log_probs_for_tokens = (
        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)
    )
    return log_probs_for_tokens


pred_log_probs = get_log_probs(demo_logits, tokens)
print(f"Avg cross entropy loss: {-pred_log_probs.mean():.4f}")
print(
    f"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}"
)
print(f"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}")

# %%

### TRAINING

model_cfg = Config(
    debug=False,
    d_model=256,
    n_heads=4,
    d_head=64,
    d_mlp=1024,
    n_layers=2,
    n_ctx=256,
    d_vocab=reference_gpt2.cfg.d_vocab,
)

model = DemoTransformer(model_cfg)

# %%


@dataclass
class TransformerTrainingArgs:
    batch_size: int = 16
    epochs: int = 10
    max_steps_per_epoch: Optional[int] = None
    lr: float = 1e-3
    weight_decay: float = 1e-2
    wandb_project: str = "day1-demotransformer"
    wandb_name = None


args = TransformerTrainingArgs()
# %%

dataset = datasets.load_dataset("NeelNanda/pile-10k", split="train").remove_columns(
    "meta"
)
assert isinstance(dataset, datasets.Dataset)
print(dataset)
print(dataset[0]["text"][:100])

# %%

tokenized_dataset = tokenize_and_concatenate(
    dataset,
    reference_gpt2.tokenizer,  # type: ignore
    streaming=False,
    max_length=model.cfg.n_ctx,
    column_name="text",
    add_bos_token=True,
    num_proc=4,
)

# %%
dd = tokenized_dataset.train_test_split(test_size=1000)


def collate(b):
    return default_collate(b)["tokens"]


train_loader = DataLoader(
    dd["train"],  # type: ignore
    batch_size=args.batch_size,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
    collate_fn=collate,
)
test_loader = DataLoader(
    dd["test"],  # type: ignore
    batch_size=args.batch_size,
    shuffle=False,
    num_workers=4,
    pin_memory=True,
    collate_fn=collate,
)

# %%

first_batch = train_loader.dataset[: args.batch_size]
print(first_batch.keys())
print(first_batch["tokens"].shape)
# %%


class TransformerTrainer:
    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):
        super().__init__()
        self.model = model
        self.args = args
        self.optimizer = t.optim.AdamW(
            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay
        )

    def training_step(self, tokens: Int[Tensor, "batch seq"]) -> Float[Tensor, ""]:
        tokens = tokens.to(device)
        self.optimizer.zero_grad()
        logits = self.model(tokens)
        # logits = t.randn_like(logits, device=device, requires_grad=True)
        loss = -get_log_probs(logits, tokens).mean()
        loss.backward()
        t.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()

        return loss

    @t.inference_mode()
    def validation_step(
        self, tokens: Int[Tensor, "batch seq"]
    ) -> Tuple[Float[Tensor, ""], Int[Tensor, "batch seq"]]:
        tokens = tokens.to(device)
        logits = self.model(tokens)

        log_probs = get_log_probs(logits, tokens)
        loss = -log_probs

        pred_tokens = logits.argmax(dim=-1)
        correct = (tokens[:, 1:] == pred_tokens[:, :-1]).bool()
        return loss, correct

    def train(self):
        if self.args.max_steps_per_epoch is not None:
            steps_per_epoch = self.args.max_steps_per_epoch
        else:
            steps_per_epoch = len(train_loader)

        wandb.init(
            project=self.args.wandb_project,
            name=self.args.wandb_name,
            config=vars(self.args),
        )
        samples = []
        progress_bar = tqdm(total=self.args.epochs * steps_per_epoch)
        global_step = 0
        for epoch in range(self.args.epochs):
            for step, batch in enumerate(train_loader):
                self.model.train()
                if (
                    self.args.max_steps_per_epoch is not None
                    and step == self.args.max_steps_per_epoch
                ):
                    break
                loss = self.training_step(batch)
                wandb.log({"loss": loss.item()}, step=global_step)
                progress_bar.set_description(f"Epoch {epoch}")
                progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

                if global_step % 500 == 0:
                    self.model.eval()

                    sample = generate(
                        "I am a large language model. My objective is:",
                        model,
                        reference_gpt2.tokenizer,
                        n=25,
                        top_p=50,
                        temp=0.7,
                    )[-1]
                    samples.append((global_step, sample))
                    wandb.log(
                        {
                            "samples": wandb.Table(
                                data=samples, columns=["step", "sample"]
                            )
                        }
                    )

                    val_losses = []
                    val_accuracies = []
                    for batch in test_loader:
                        loss, correct = self.validation_step(batch)
                        val_losses.append(loss)
                        val_accuracies.append(correct)

                    val_loss = t.cat(val_losses).mean()
                    val_accuracy = t.cat(val_accuracies).float().mean()
                    wandb.log(
                        {"val_loss": val_loss.item(), "accuracy": val_accuracy.item()},
                        step=global_step,
                    )

                progress_bar.update()
                global_step += 1
        wandb.finish()


model = DemoTransformer(model_cfg).to(device)
args = TransformerTrainingArgs(batch_size=1024, epochs=1, max_steps_per_epoch=501)
trainer = TransformerTrainer(args, model)
trainer.train()
# %%

toks = tokenized_dataset[:]["tokens"].flatten()

d_vocab = model.cfg.d_vocab
freqs = t.bincount(toks, minlength=d_vocab)
probs = freqs.float() / freqs.sum()
distn = t.distributions.Categorical(probs=probs)
entropy = distn.entropy()

print("Entropy:", entropy.item())
# %%

## SAMPLING

model_cfg = Config()
model = DemoTransformer(model_cfg).to(device)
model.load_state_dict(reference_gpt2.state_dict(), strict=False)

tokenizer = reference_gpt2.tokenizer
assert isinstance(tokenizer, GPT2TokenizerFast)


class TransformerSampler:

    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):
        self.model = model
        self.cfg = model.cfg
        self.tokenizer = tokenizer

    @t.inference_mode()
    def sample(self, prompt: str, max_tokens=100, verbose=False, **kwargs) -> str:
        self.model.eval()
        tokens = self.tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)  # type: ignore
        for _ in range(max_tokens):
            logits = model(tokens)[0, -1]
            next_token = self.sample_next_token(tokens[0], logits, **kwargs)
            tokens = t.cat([tokens, t.tensor([next_token], device=device)[None]], dim=1)
            if verbose:
                print(self.tokenizer.decode(input_ids), end="\r")
            if next_token == self.tokenizer.eos_token_id:
                break
        return self.tokenizer.decode(tokens[0])

    @staticmethod
    def sample_next_token(
        input_ids,
        logits,
        temperature=1.0,
        top_k=0,
        top_p=0.0,
        frequency_penalty=0.0,
        seed=None,
    ) -> int:
        assert input_ids.ndim == 1, "input_ids should be a 1D sequence of token ids"
        assert temperature >= 0, "Temperature should be non-negative"
        assert 0 <= top_p <= 1.0, "Top-p must be a probability"
        assert 0 <= top_k, "Top-k must be non-negative"
        assert not (
            top_p != 0 and top_k != 0
        ), "At most one of top-p and top-k supported"

        if seed is not None:
            t.manual_seed(seed)

        if temperature == 0:
            return TransformerSampler.greedy_search(logits)
        elif temperature != 1.0:
            logits = TransformerSampler.apply_temperature(logits, temperature)

        if frequency_penalty != 0.0:
            logits = TransformerSampler.apply_frequency_penalty(
                input_ids, logits, frequency_penalty
            )

        if top_k > 0:
            return TransformerSampler.sample_top_k(logits, top_k)
        if top_p > 0.0:
            return TransformerSampler.sample_top_p(logits, top_p)

        return TransformerSampler.sample_basic(logits)

    @staticmethod
    def greedy_search(logits: Float[Tensor, "d_vocab"]) -> int:
        out = logits.argmax().item()
        assert isinstance(out, int)
        return out

    @staticmethod
    def apply_temperature(logits: Float[Tensor, "d_vocab"], temperature: float):
        return logits / temperature

    @staticmethod
    def apply_frequency_penalty(
        input_ids: Int[Tensor, "seq_len"],
        logits: Float[Tensor, "d_vocab"],
        freq_penalty: float,
    ) -> Float[Tensor, "d_vocab"]:
        (vocab_size,) = logits.shape
        id_freqs = t.bincount(input_ids, minlength=vocab_size)
        return logits - freq_penalty * id_freqs

    @staticmethod
    def sample_basic(logits: Float[Tensor, "d_vocab"]) -> int:
        probs = F.softmax(logits, dim=-1)
        out = t.multinomial(probs, 1).item()
        assert isinstance(out, int)
        return out

    @staticmethod
    def sample_top_k(logits: Float[Tensor, "d_vocab"], k: int) -> int:
        vals, idxs = logits.topk(k)
        out = idxs[TransformerSampler.sample_basic(vals)].item()
        assert isinstance(out, int)
        return out

    @staticmethod
    def sample_top_p(
        logits: Float[Tensor, "d_vocab"], top_p: float, min_tokens_to_keep: int = 1
    ) -> int:
        sorted_logits, idxs = logits.sort(descending=True, stable=True)
        cumsum_probs = sorted_logits.softmax(-1).cumsum(-1)
        n_keep = t.searchsorted(cumsum_probs, top_p, side="right").item() + 1
        n_keep = max(n_keep, min_tokens_to_keep)
        keep_idx = idxs[:n_keep]
        keep_logits = logits[keep_idx]

        sample = t.multinomial(F.softmax(keep_logits, dim=-1), 1).item()
        assert isinstance(sample, int)
        out = keep_idx[sample].item()
        assert isinstance(out, int)
        return out


sampler = TransformerSampler(model, tokenizer)

prompt = "Jingle bells, jingle bells, jingle all the way"
print(f"Greedy decoding with prompt: {prompt!r}\n")

output = sampler.sample(prompt, max_tokens=8, temperature=0.0)
print(f"Your model said: {output!r}\n")

expected = (
    "Jingle bells, jingle bells, jingle all the way up to the top of the mountain."
)
assert output == expected

print("Tests passed!")

# %%

prompt = "John and Mary went to the"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)  # type: ignore
logits = model(input_ids)[0, -1]

expected_top_5 = {
    " church": 0.0648,
    " house": 0.0367,
    " temple": 0.0145,
    " same": 0.0104,
    " Church": 0.0097,
}
frequency_of_top_5 = defaultdict(int)

N = 10_000
for _ in tqdm(range(N)):
    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits)
    frequency_of_top_5[tokenizer.decode(token)] += 1

for word in expected_top_5:
    expected_freq = expected_top_5[word]
    observed_freq = frequency_of_top_5[word] / N
    print(
        f"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}"
    )
    assert (
        abs(observed_freq - expected_freq) < 0.01
    ), "Try increasing N if this fails by a small amount."

print("Tests passed!")

# %%
logits = t.tensor([1, 2]).log()

cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)
print('A low temperature "sharpens" or "peaks" the distribution: ', cold_logits)
t.testing.assert_close(cold_logits, 1000.0 * logits)

hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)
print("A high temperature flattens the distribution: ", hot_logits)
t.testing.assert_close(hot_logits, 0.001 * logits)

print("Tests passed!")

# %%
bieber_prompt = "And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine"
input_ids = tokenizer.encode(bieber_prompt, return_tensors="pt")
logits = t.ones(tokenizer.vocab_size)
assert isinstance(input_ids, Tensor)
penalized_logits = TransformerSampler.apply_frequency_penalty(
    input_ids.squeeze(), logits, 2.0
)

assert (
    penalized_logits[5156].item() == -11
), "Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11"
assert (
    penalized_logits[14801].item() == -5
), "Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5"

print("Tests passed!")
# %%
sampler = TransformerSampler(model, tokenizer)

N_RUNS = 1
your_prompt = "Jingle bells, jingle bells, jingle all the way"
cases = [
    ("High freq penalty", dict(frequency_penalty=100.0)),
    ("Negative freq penalty", dict(frequency_penalty=-3.0)),
    ("Too hot!", dict(temperature=2.0)),
    ("Pleasantly cool", dict(temperature=0.7)),
    ("Pleasantly warm", dict(temperature=0.9)),
    ("Too cold!", dict(temperature=0.01)),
]

table = Table("Name", "Kwargs", "Output", title="Sampling - Manual Testing")

for name, kwargs in cases:
    for i in range(N_RUNS):
        output = sampler.sample(your_prompt, max_tokens=24, **kwargs)
        table.add_row(name, repr(kwargs), repr(output) + "\n")

rprint(table)

# %%

prompt = "John and Mary went to the"
input_ids = tokenizer.encode(prompt, return_tensors="pt")
assert isinstance(input_ids, Tensor)
input_ids = input_ids.to(device)
logits = model(input_ids)[0, -1]

expected_top_5 = {
    " church": 0.0648,
    " house": 0.0367,
    " temple": 0.0145,
    " same": 0.0104,
    " Church": 0.0097,
}
topk_5_sum = sum(expected_top_5.values())

observed_freqs = defaultdict(int)

N = 10000
for _ in tqdm(range(N)):
    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_k=5)
    observed_freqs[tokenizer.decode(token)] += 1

for word in expected_top_5:
    expected_freq = expected_top_5[word] / topk_5_sum
    observed_freq = observed_freqs[word] / N
    print(
        f"Word: {word!r:<9}. Expected freq = {expected_freq:.4f}, observed freq = {observed_freq:.4f}"
    )
    assert (
        abs(observed_freq - expected_freq) < 0.015
    ), "Try increasing N if this fails by a small amount."
# %%

sampler = TransformerSampler(model, tokenizer)

your_prompt = "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
output = sampler.sample(your_prompt, temperature=0.7, top_k=40, max_tokens=64)
rprint(f"Your model said:\n\n[bold dark_orange]{output}")
# %%

prompt = "John and Mary went to the"
input_ids = tokenizer.encode(prompt, return_tensors="pt")
assert isinstance(input_ids, Tensor)
input_ids = input_ids.to(device)
logits = model(input_ids)[0, -1]

expected_top_10pct = {
    " church": 0.0648,
    " house": 0.0367,  # These are the two most likely tokens, and add up to >10%
}
top_10pct_sum = sum(expected_top_10pct.values())

observed_freqs = defaultdict(int)

N = 10000
for _ in tqdm(range(N)):
    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)
    observed_freqs[tokenizer.decode(token)] += 1

for word in expected_top_10pct:
    expected_freq = expected_top_10pct[word] / top_10pct_sum
    observed_freq = observed_freqs[word] / N
    print(
        f"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}"
    )
    assert (
        abs(observed_freq - expected_freq) < 0.01
    ), "Try increasing N if this fails by a small amount."
# %%

sampler = TransformerSampler(model, tokenizer)

your_prompt = "Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for"
output = sampler.sample(your_prompt, temperature=0.7, top_p=0.95, max_tokens=64)
rprint(f"Your model said:\n\n[bold dark_orange]{output}")
# %%


@dataclass
class Beams:
    model: DemoTransformer
    tokenizer: GPT2TokenizerFast
    logprob_sums: Float[Tensor, "batch"]
    tokens: Int[Tensor, "batch seq"]

    def new_beams(self, logprob_sums, tokens) -> Beams:
        return Beams(self.model, self.tokenizer, logprob_sums, tokens)

    def __getitem__(self, idx) -> Beams:
        return self.new_beams(self.logprob_sums[idx], self.tokens[idx])

    @property
    def logprobs_and_completions(self) -> List[Tuple[float, str]]:
        return [
            (logprob_sum.item(), self.tokenizer.decode(tokens))
            for logprob_sum, tokens in zip(self.logprob_sums, self.tokens)
        ]

    def generate(
        self, toks_per_beam: int, no_repeat_ngram_size: Optional[int] = None
    ) -> Beams:
        logits = self.model(self.tokens)[:, -1, :]
        log_probs = logits.log_softmax(dim=-1)
        log_probs, idxs = self.get_topk_non_repeating(
            log_probs, no_repeat_ngram_size, toks_per_beam
        )

        new_log_probs = (self.logprob_sums[:, None] + log_probs).flatten()

        prep_tokens = self.tokens.repeat_interleave(toks_per_beam, dim=0)
        new_tokens = t.cat([prep_tokens, idxs.flatten()[:, None]], dim=1)

        return self.new_beams(new_log_probs, new_tokens)

    # def get_topk_non_repeating(
    #     self,
    #     logprobs: Float[Tensor, "batch d_vocab"],
    #     no_repeat_ngram_size: int,
    #     k: int
    # ) -> Tuple[Float[Tensor, "k"], Int[Tensor, "k"]]:
    #     ngrams = self.tokens.unfold(1, no_repeat_ngram_size, 1).unique(dim=0)
    #     seq_ngram = self.tokens[-no_repeat_ngram_size+1:]
    #     ngrams

    def get_topk_non_repeating(
        self,
        logprobs: Float[Tensor, "batch d_vocab"],
        no_repeat_ngram_size: Optional[int],
        k: int,
    ) -> Tuple[Float[Tensor, "k"], Int[Tensor, "k"]]:
        """
        logprobs:
            tensor of the log-probs for the next token
        no_repeat_ngram_size:
            size of ngram to avoid repeating
        k:
            number of top logits to return, for each beam in our collection

        Returns:
            equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure
            that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`
            which has already appeared in `self.tokens`.
        """
        batch, seq_len = self.tokens.shape
        neg_inf = t.tensor(-1.0e4).to(device)

        # If completion isn't long enough for a repetition, or we have no restructions, just return topk
        if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size - 1):
            # Otherwise, we need to check for ngram repetitions
            # First, get the most recent `no_repeat_ngram_size-1` tokens
            last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size - 1) :]
            # Next, find all the tokens we're not allowed to generate (by going iterating through past ngrams and seeing if those ngram prefixes match the last one)
            for i in range(seq_len - (no_repeat_ngram_size - 1)):
                ngrams = self.tokens[:, i : i + no_repeat_ngram_size]  # (batch, ngram)
                ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(
                    -1
                )  # (batch,)
                ngram_end_tokens = ngrams[:, [-1]]  # (batch, 1)
                # Fill logprobs with neginf wherever the ngrams are repeated
                logprobs[range(batch), ngram_end_tokens] = t.where(
                    ngrams_are_repeated,
                    neg_inf,
                    logprobs[range(batch), ngram_end_tokens],
                )

        # Finally, get our actual tokens
        return logprobs.topk(k=k, dim=-1)

    def filter(self, num_beams: int) -> Tuple[Beams, Beams]:
        eos_mask = self.tokens[:, -1] == self.tokenizer.eos_token_id
        early_term_tokens = self.tokens[eos_mask]
        early_term_logprob_sums = self.logprob_sums[eos_mask]

        tokens = self.tokens[~eos_mask]
        logprob_sums = self.logprob_sums[~eos_mask]

        best_idxs = t.argsort(logprob_sums, descending=True)[:num_beams]
        tokens = tokens[best_idxs]
        logprob_sums = logprob_sums[best_idxs]

        return self.new_beams(logprob_sums, tokens), self.new_beams(
            early_term_logprob_sums, early_term_tokens
        )

    def print(self, title="Best completions", max_print_chars=80) -> None:
        if len(self.tokens) == 0:
            return
        table = Table("logitsum", "completion", title=title)
        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):
            text = self.tokenizer.decode(tokens)
            if len(repr(text)) > max_print_chars:
                text = (
                    text[: int(0.3 * max_print_chars)]
                    + " ... "
                    + text[-int(0.7 * max_print_chars) :]
                )
            table.add_row(f"{logprob_sum:>8.3f}", repr(text))
        rprint(table)


beams = Beams(
    model,
    tokenizer,
    logprob_sums=t.tensor([-10.0, -15.0, -20.0]).to(device),
    tokens=t.tensor(
        [
            [5661, 318, 262, 2368],
            [5661, 318, 262, 1218],
            [5661, 318, 262, 717],
        ]
    ).to(device),
)

beams.print()

print("Testing generate, without no_repeat_ngram_size argument:")
new_beams = beams.generate(toks_per_beam=2)
new_beams.print()
assert new_beams.logprobs_and_completions[0][1] == "this is the third time"

print("Testing generate, with no_repeat_ngram_size argument:")
bigram_beams = Beams(
    model,
    tokenizer,
    logprob_sums=t.tensor([-0.0]).to(device),
    tokens=t.tensor([[530, 734, 530, 734]]).to(device),
    # tokens are " one two one two"
)

# With no_repeat_ngram_size=1, should not generate the token " one" or " two"
new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=1)
new_bigram_beams.print()
assert all(
    [
        not (completion[1].endswith(" one") or completion[1].endswith(" two"))
        for completion in new_bigram_beams.logprobs_and_completions
    ]
)

# With no_repeat_ngram_size=2, it can generate " two" (which it should), but not " one"
new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=2)
new_bigram_beams.print()
assert all(
    [
        not completion[1].endswith(" one")
        for completion in new_bigram_beams.logprobs_and_completions
    ]
)
assert any(
    [
        not completion[1].endswith(" two")
        for completion in new_bigram_beams.logprobs_and_completions
    ]
)

print("All tests for `generate` passed!")

logprob_sums = t.tensor([-1.0, -2.0]).to(device)
tokens = t.tensor([[19485, 13], [19485, tokenizer.eos_token_id]]).to(device)

beams_with_eos = Beams(model, tokenizer, logprob_sums, tokens)
best_beams, early_terminations = beams_with_eos.filter(2)

t.testing.assert_close(best_beams.logprob_sums, logprob_sums[[0]])
t.testing.assert_close(best_beams.tokens, tokens[[0]])

assert early_terminations.logprobs_and_completions == [
    (-2.0, "Stop" + tokenizer.eos_token)
]

print("All tests for `filter` passed!")

# %%


@t.inference_mode()
def beam_search(
    self: TransformerSampler,
    prompt: str,
    num_return_sequences: int,
    num_beams: int,
    max_new_tokens: int,
    no_repeat_ngram_size: Optional[int] = None,
    verbose=False,
) -> List[Tuple[float, Tensor]]:
    assert num_return_sequences <= num_beams
    self.model.eval()
    tokens = self.tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)  # type: ignore
    logprob_sums = t.zeros(tokens.size(0), device=device)
    beams = Beams(self.model, self.tokenizer, logprob_sums, tokens)
    out = []
    for _ in tqdm(range(max_new_tokens)):
        beams = beams.generate(
            toks_per_beam=num_beams, no_repeat_ngram_size=no_repeat_ngram_size
        )
        beams, term_beam = beams.filter(num_beams)
        term_logprobs_completions = term_beam.logprobs_and_completions
        if len(term_logprobs_completions):
            out.extend(term_logprobs_completions)

    out.extend(beams.logprobs_and_completions)
    print(out)
    out.sort(reverse=False)
    return out[:num_return_sequences]


TransformerSampler.beam_search = beam_search

sampler = TransformerSampler(model, tokenizer)

prompt = "The ships hung in the sky in much the same way that"
orig_len = len(tokenizer.encode(prompt))

final_logitsums_and_completions = sampler.beam_search(
    prompt=prompt,
    num_return_sequences=3,
    num_beams=40,
    max_new_tokens=60,
    no_repeat_ngram_size=2,
    verbose=False,
)

# Print all the best output
for logprob_sum, text in final_logitsums_and_completions:
    avg_logprob_as_prob = (
        t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()
    )
    print(
        "=" * 25
        + f" Avg logprob (as probability) = {avg_logprob_as_prob:.3f} "
        + "=" * 25
    )
    rprint("Best output:\n\n[bold dark_orange]" + text)

# %%
